# Embedding Encoding

## Distributional Encoders (2/8)

### Reading

* [A Neural Probabilistic Language Model](https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf), Bengio et al., NIPS 2000.
* [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 
Mikolov et al., NIPS, 2013.
* [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162), Pennington et al., EMNLP, 2014.*
* [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010), Bojanowski et al., TACL, 2017.


## Contextualized Encoders (2/10)

### Reading

* [_context2vec_: Learning Generic Context Embedding with Bidirectional LSTM](http://www.aclweb.org/anthology/K16-1006), Melamud et al., CoNLL 2016.
* [Deep Contextualized Word Representations](https://aclweb.org/anthology/N18-1202), Peters et al., NAACL 2018.
* [Regularizing and Optimizing LSTM Language Models](https://openreview.net/forum?id=SyyGPP0TZ), Merity et al., ICLR, 2018.*
* [Contextual String Embeddings for Sequence Labeling](https://www.aclweb.org/anthology/C18-1139), Akbik et al., COLING 2018


## Transformer Encoders (2/15)

### Reading

* [Attention is All you Need](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), Vaswani et al., NIPS 2017.
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/), Devlin et al., NAACL 2019.
* [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692), Liu et al., arXiv 2019.
* [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB), Clark et al., ICLR 2020.*
